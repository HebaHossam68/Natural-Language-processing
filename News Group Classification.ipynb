{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2dca102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import string\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import PurePath,PureWindowsPath\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from itertools import chain\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "862a30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_metadata(lines):\n",
    "    for i in range(len(lines)):\n",
    "        if(lines[i] == '\\n'):\n",
    "            start = i+1\n",
    "            break\n",
    "    new_lines = lines[start:]\n",
    "    return new_lines\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    \"\"\"\n",
    "        Remove the html in sample text\n",
    "    \"\"\"\n",
    "    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n",
    "    return re.sub(html, \"\", text)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def lemmaLower(words):\n",
    "    #lemmatization\n",
    "    words = words.split()\n",
    "    #we also normalize the cases of our words\n",
    "    words = [word.lower() for word in words]\n",
    "    #pos\n",
    "    #tags = pos_tag(words)\n",
    "    #tags = list(tags)\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    #removing stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "  \n",
    "    joined_string = ' '.join(words)\n",
    "    joined_string = re.sub (r'\\b\\w\\b','', joined_string)\n",
    "    \n",
    "    return joined_string\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def prepare(path):\n",
    "    f = open(path, 'r')\n",
    "    text_lines = f.readlines()\n",
    "    \n",
    "    #removing the meta-data at the top of each document\n",
    "    text_lines = remove_metadata(text_lines)\n",
    "    \n",
    "    #initiazing an array to hold all the words in a document\n",
    "    doc_words = []\n",
    "   \n",
    "    #traverse over all the lines and tokenize each one with the help of helper function: tokenize_sentence\n",
    "    \n",
    "    doc_words.append(text_lines)\n",
    "    \n",
    "\n",
    "    return doc_words\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def Text_Cleaning(data):\n",
    "    \n",
    "    data= data.replace('\\\\n','')\n",
    "    data= data.replace('\\\\t','')\n",
    "    \n",
    "    #replace any non-charachter by space\n",
    "    data  = re.sub('[^a-zA-Z]', ' ', data)\n",
    "  \n",
    "    #remove html tages\n",
    "    data = remove_html(data)\n",
    "    #remove punctuation\n",
    "    punctuationfree = \"\".join([i for i in data if i not in string.punctuation])\n",
    "    data = punctuationfree\n",
    "    data = [re.sub (r'\\b\\w\\b','', data)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a9f1b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20_newsgroups',\n",
       " 'alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "my_path = r\"C:\\Users\\future\\Desktop\\20-news\\20_newsgroups\"\n",
    "os.chdir(my_path)\n",
    "cwd = os.getcwd()\n",
    "#creating a list of folder names to make valid pathnames later\n",
    "folders = [f for f in listdir(my_path)]\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de323c8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] The directory name is invalid: 'C:\\\\Users\\\\future\\\\Desktop\\\\20-news\\\\20_newsgroups\\\\20_newsgroups'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6948\\2973019226.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfolders\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mfolder_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] The directory name is invalid: 'C:\\\\Users\\\\future\\\\Desktop\\\\20-news\\\\20_newsgroups\\\\20_newsgroups'"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "for folder_name in folders:\n",
    "    folder_path = join(my_path, folder_name)\n",
    "    files.append([f for f in listdir(folder_path)])\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathname_list = []\n",
    "for i in range(len(folders)):\n",
    "    for f in files[i]:\n",
    "        pathname_list.append(join(my_path, join(folders[i], f)))\n",
    "\n",
    "pathname_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(pathname_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5dbe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "for folder_name in folders:\n",
    "    folder_path = join(my_path, folder_name)\n",
    "    num_of_files= len(listdir(folder_path))\n",
    "    for i in range(num_of_files):\n",
    "        Y.append(folder_name)\n",
    "        \n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d8b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aa3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X\n",
    "list_of_words = []\n",
    "\n",
    "for document in pathname_list:\n",
    "        list_of_words.append(prepare(str(document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4091080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert it into dataframe and create Y\n",
    "df = pd.DataFrame(list_of_words, columns=['documents'])\n",
    "df['category'] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a5c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Cleaning\n",
    "df[\"documents\"] = df[\"documents\"].apply(lambda x: Text_Cleaning(str(x)))\n",
    "#remove extra spaces in text\n",
    "df[\"documents\"] = df[\"documents\"].apply(lambda x: [\" \".join(text.split()) for text in x])\n",
    "df[\"documents\"] = df[\"documents\"].apply(lambda x: \" \".join(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spiliting data into train & test\n",
    "#return the train test split to 0.20\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['documents'][:5000], df['category'][:5000], random_state=0, test_size=0.20)\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc20ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english',preprocessor = lemmaLower , max_features=55900 , min_df = 0.0001, max_df = 0.97)\n",
    "tfidfvectorizer.fit(X_train)\n",
    "X_train_tfidf = tfidfvectorizer.transform(X_train).toarray()\n",
    "X_test_tfidf = tfidfvectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4580239",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()  \n",
    "y_pred = gnb.fit(X_train_tfidf, y_train).predict(X_test_tfidf)\n",
    "\n",
    "print(\"Naive Bayes accuracy: \", gnb.score( X_test_tfidf, y_test)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clff = SVC(kernel='linear')\n",
    "clff.fit(X_train_tfidf, y_train)\n",
    "accuracy = clff.score(X_test_tfidf, y_test)\n",
    "\n",
    "print(f\"SVM Accuracy: {accuracy*100}\")\n",
    " #Accuracy linear: 85.575\n",
    " #Accuracy poly: 78.64999999999999\n",
    " #Accuracy rbf: 85.35000000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a698824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN model\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1500)  # Specify the number of neighbors (K)\n",
    "knn.fit(X_train_tfidf, y_train)\n",
    "accuracy_knn = knn.score(X_test_tfidf, y_test)\n",
    "\n",
    "print(f\"KNN Accuracy: {accuracy_knn*100}\")\n",
    "#KNN Accuracy: 69.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab264006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier model\n",
    "\n",
    "#gbm = GradientBoostingClassifier()\n",
    "#gbm.fit(X_train_tfidf, y_train)\n",
    "#accuracy_gbm = gbm.score(X_test_tfidf, y_test)\n",
    "\n",
    "#print(f\"GBM Accuracy: {accuracy_gbm*100}\")\n",
    "#GBM Accuracy: 75.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4eb03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTree model\n",
    "\n",
    "dtc = DecisionTreeClassifier(criterion=\"entropy\", random_state=100, max_depth=3 , min_samples_leaf=5)\n",
    "dtc = dtc.fit(X_train_tfidf,y_train)\n",
    "y_pred = dtc.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Decision tree Accuracy:\",accuracy_score(y_test, y_pred)*100)\n",
    "\n",
    "\n",
    "# Predicting the values of test data\n",
    "#print(\"Classification report - \\n\", classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf470c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest model\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100}\")\n",
    "#Accuracy: 78.27499999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6074ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log=LogisticRegression(random_state=16)\n",
    "log.fit(X_train_tfidf,y_train)\n",
    "x_test_pred=log.predict(X_test_tfidf)\n",
    "test_accuracy=accuracy_score(x_test_pred,y_test)\n",
    "\n",
    "print(\"accuracy of testing data by Logistic Regression is : \",test_accuracy*100)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcfa5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib_file=\"20_newsgroups\"\n",
    "joblib.dump(log,joblib_file)\n",
    "\n",
    "loaded_model=joblib.load(open(joblib_file,'rb'))\n",
    "x_test_pred=loaded_model.predict(X_test_tfidf)\n",
    "result=np.round(accuracy_score(y_test,x_test_pred),2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ea85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib_file=\"20_newsgroups\"\n",
    "joblib.dump(clf,joblib_file)\n",
    "\n",
    "loaded_model=joblib.load(open(joblib_file,'rb'))\n",
    "x_test_pred=loaded_model.predict(X_test_tfidf)\n",
    "result=np.round(accuracy_score(y_test,x_test_pred),2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa39ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib_file=\"20_newsgroups\"\n",
    "joblib.dump(gnb,joblib_file)\n",
    "\n",
    "loaded_model=joblib.load(open(joblib_file,'rb'))\n",
    "x_test_pred=loaded_model.predict(X_test_tfidf)\n",
    "result=np.round(accuracy_score(y_test,x_test_pred),2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d7b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib_file=\"20_newsgroups\"\n",
    "joblib.dump(knn,joblib_file)\n",
    "\n",
    "loaded_model=joblib.load(open(joblib_file,'rb'))\n",
    "x_test_pred=loaded_model.predict(X_test_tfidf)\n",
    "result=np.round(accuracy_score(y_test,x_test_pred),2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a317523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib_file=\"20_newsgroups\"\n",
    "joblib.dump(clff,joblib_file)\n",
    "\n",
    "loaded_model=joblib.load(open(joblib_file,'rb'))\n",
    "x_test_pred=loaded_model.predict(X_test_tfidf)\n",
    "result=np.round(accuracy_score(y_test,x_test_pred),2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a3253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import joblib\n",
    "#joblib_file=\"20_newsgroups\"\n",
    "#joblib.dump(gbm,joblib_file)\n",
    "\n",
    "#loaded_model=joblib.load(open(joblib_file,'rb'))\n",
    "#x_test_pred=loaded_model.predict(X_test_tfidf)\n",
    "#result=np.round(accuracy_score(y_test,x_test_pred),2)\n",
    "#print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
